\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[preprint]{main}


% to compile a preprint version, e.g., for submission to arxiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}	%image
\usepackage{amsmath, nccmath}	%align
\usepackage{tikz}
\usepackage[fleqn,tbtags]{mathtools}
\usepackage{cancel}
\usepackage[mathscr]{euscript}
\usepackage{tabto}
\usepackage{mathtools}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D_{KL}\infdivx}

\input{zsupport/algo}
\input{zsupport/english}
\input{zsupport/figure}
\input{zsupport/linking}
\input{zsupport/math}
\input{zsupport/mathenv}
\input{zsupport/mathvar}
\input{zsupport/refer}
\input{zsupport/table}
\input{zsupport/misc}

% https://www.overleaf.com/learn/latex/Questions/How_do_I_add_additional_author_names_and_affiliations_to_my_paper%3F
\usepackage{authblk}

\usepackage[yyyymmdd,hhmmss]{datetime}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Fondasi Matematika dari Denoising Diffusion Probabilistic Models}

\author{
	Dimas Tri Kurniawan\\
	\texttt{dimas.tri01@ui.ac.id}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle


\begin{abstract}
Denoising Diffusion Probabilistic Models (DDPM) merupakan salah satu pendekatan terbaru dalam generasi citra yang telah menunjukkan hasil yang mengesankan dalam menghasilkan citra berkualitas tinggi. Model ini didasarkan pada prinsip difusi terbalik, yaitu mengubah distribusi data asli menjadi distribusi noise, lalu memulihkannya kembali ke data asli melalui proses denoising bertahap. Dalam paper ini, penulis ingin mengulas lebih dalam tentang fondasi matematika dari DDPM. Penulis menganggap bahwa masih terdapat kekurangan dalam penjelasan dari fondasi matematika yang ada pada paper DDPM. Oleh karena itu, penulis tertarik untuk mengulas berbagai rumus yang digunakan sebagai fondasi dari paper tersebut.
\end{abstract}


\section{Pendahuluan}


Denoising Diffusion Probabilistic Models (yang selanjutnya kita sebut Diffusion Models, untuk mempersingkat) telah muncul sebagai salah satu metode terbaru dan paling menjanjikan dalam generasi citra dan data berbasis probabilistik. Sejak pertama kali diperkenalkan, DDPM telah menunjukkan kemajuan yang signifikan dalam kualitas gambar yang dihasilkan, melampaui metode generatif lain seperti Generative Adversarial Networks (GANs) dalam hal stabilitas dan kemampuan untuk menghasilkan citra dengan detail yang sangat tinggi. Metode ini beroperasi melalui proses difusi, yang mengubah data asli menjadi noise secara bertahap, kemudian berusaha memulihkan data asli dari noise tersebut menggunakan model probabilistik. Pendekatan ini menawarkan cara yang baru dan menarik untuk memahami bagaimana model dapat mengatasi tantangan dalam generasi data, terutama dalam konteks pengolahan citra.

Meskipun hasil empiris DDPM sangat menjanjikan, sebagian besar literatur yang ada cenderung fokus pada implementasi algoritmik dan teknik optimisasi tanpa memberikan penjelasan yang mendalam mengenai fondasi matematis dari model ini. Padahal, pemahaman yang lebih mendalam tentang aspek matematis DDPM sangat penting untuk meningkatkan pemahaman kita tentang bagaimana model ini berfungsi, serta untuk membuka potensi aplikasi lebih lanjut dalam berbagai domain, seperti pemodelan distribusi data dan rekonstruksi citra.

Paper ini bertujuan untuk mengisi kekosongan tersebut dengan menjelaskan secara rinci dasar-dasar matematis yang mendasari Denoising Diffusion Probabilistic Models. Kami akan membahas bagaimana proses difusi terbalik bekerja dalam konteks probabilistik, serta menguraikan teori probabilitas yang relevan, termasuk peran distribusi noise dan pembelajaran berbasis optimisasi dalam meningkatkan kinerja model. Selain itu, kami juga akan menjelaskan tantangan utama yang dihadapi dalam penerapan DDPM, serta potensi arah pengembangan selanjutnya untuk model ini.

Dengan memberikan wawasan yang lebih mendalam mengenai dasar-dasar matematis dari DDPM, diharapkan pembaca dapat memperoleh pemahaman yang lebih komprehensif tentang bagaimana model ini bekerja dan bagaimana potensi teknologinya dapat dimanfaatkan di masa depan.


\section{Latar Belakang}


Diffusion models terdiri dari 2 proses utama, yaitu Forward/Diffusion Process dan Reverse Diffusion Process.


\subsection{Forward Process}


Forward process/diffusion process adalah proses di mana kita menambahkan noise secara berkala ke sebuah gambar sampai gambar tersebut konvergen ke distribusi gaussian dengan mean 0 dan variance 1. Dengan kata lain, gambar tersebut akan menjadi pure noise setelah t langkah. Forward process didefinisikan dengan Markov chain yang menambahkan Gaussian noise secara berkala:
\\
\begin{equation}
q(\textbf{x}_{1:T}|\textbf{x}_{0}) \coloneq \displaystyle \prod_{t=1}^{T} q(\textbf{x}_{t}|\textbf{x}_{t-1})
\end{equation}
\\
Persamaan (1) bermaksud bahwa kita ingin mencari distribusi gambar yang telah diberikan noise $ \textbf{x}_{1} $ sampai $ \textbf{x}_{T} $ jika diketahui gambar asli $ \textbf{x}_{0} $. Karena menggunakan asumsi Markov, gambar/state sebelumnya tidak diperlukan sehingga persamaan yang seharusnya:
\\\\
$ q(\textbf{x}_{1:T}|\textbf{x}_{0}) \coloneq  \frac{q(\textbf{x}_{T}|\textbf{x}_{T-1},\textbf{x}_{T-2},\textbf{x}_{T-3},...,\textbf{x}_{0}) \ q(\textbf{x}_{T-1}|\textbf{x}_{T-2},\textbf{x}_{T-3},\textbf{x}_{T-4},...,\textbf{x}_{0}) \ ... \ q(\textbf{x}_{0})  } {  q(\textbf{x}_{0}) } $,
\\\\
dapat kita sederhanakan dengan menghilangkan state sebelumnya:
\begin{tabbing} 
\hspace{4.75em} $ \coloneq \frac{q(\textbf{x}_{T}|\textbf{x}_{T-1},\cancel{\textbf{x}_{T-2}},\cancel{\textbf{x}_{T-3}},\cancel{...},\cancel{\textbf{x}_{0}}) \ q(\textbf{x}_{T-1}|\textbf{x}_{T-2},\cancel{\textbf{x}_{T-3}},\cancel{\textbf{x}_{T-4}},\cancel{...},\cancel{\textbf{x}_{0}}) \ ... \ \cancel{ q(\textbf{x}_{0}) } } {  \cancel{ q(\textbf{x}_{0}) } } $ 
\end{tabbing}
menjadi:
\begin{tabbing} 
\hspace{4.75em} $ \coloneq \displaystyle \prod_{t=1}^{T} q(\textbf{x}_{t}|\textbf{x}_{t-1}) $.
\end{tabbing}
Forward process menambahkan Gaussian noise secara berkala berdasarkan linear variance schedule $ \beta_1, \beta_2, \beta_3, ..., \beta_t $:
\\\\
$  q(\textbf{x}_{t}|\textbf{x}_{t-1}) \coloneq \mathscr{N}(\textbf{x}_{t};\sqrt{1-\beta_t}\textbf{x}_{t-1},\beta_t \textbf{I}) $,
\\\\
dengan $ \textbf{x}_{t} $ adalah output, $ \sqrt{1-\beta_t} $ mean, dan $ \beta_t $ variance dari $ \textbf{x}_{t} $. Melalui reparameterization trick $ \mathscr{N}(\mu,\sigma^2) = \mu + \sigma \epsilon $, persamaan di atas dapat ditulis kembali menjadi:
\begin{tabbing} 
\hspace{4.75em} $ \coloneq \sqrt{1-\beta_t}\textbf{x}_{t-1} + \sqrt{\beta_t}\epsilon_{t-1} $.
\end{tabbing}
Didefinisikan $ \alpha_t \coloneq 1 - \beta_t $, maka persamaan di atas menjadi:
\begin{tabbing} 
\hspace{4.75em} $ \coloneq \sqrt{\alpha_t}\textbf{x}_{t-1} + \sqrt{1-\alpha_t}\epsilon_{t-1} $.
\end{tabbing}
Subtitusi $ \textbf{x}_{t-1} $:
\begin{tabbing} 
\hspace{4.75em} $ \coloneq \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}} \textbf{x}_{t-2} + \sqrt{1-\alpha_{t-1}} \epsilon_{t-2}) + \sqrt{1-\alpha_t}\epsilon_{t-1} $.
\\\\
\hspace{4.75em} $ \coloneq \sqrt{\alpha_t \alpha_{t-1}} \textbf{x}_{t-2} + \sqrt{\alpha_t - \alpha_t\alpha_{t-1}} \epsilon_{t-2} + \sqrt{1-\alpha_t}\epsilon_{t-1} $.
\end{tabbing}
Jika dua 2 gaussian noise $ \mathscr{N}(\mu,\sigma_1^2) $ dan $ \mathscr{N}(\mu,\sigma_2^2) $ ditambahkan, maka hasilnya yaitu $ \mathscr{N}(\mu,(\sigma_1^2 + \sigma_2^2)) $. Ubah 2 term terakhir:
\begin{tabbing} 
\hspace{4.75em} $ \coloneq \sqrt{\alpha_t \alpha_{t-1}} \textbf{x}_{t-2} + \sqrt{\alpha_t - \alpha_t\alpha_{t-1} + 1-\alpha_t} \epsilon $.
\\\\
\hspace{4.75em} $ \coloneq \sqrt{\alpha_t \alpha_{t-1}} \textbf{x}_{t-2} + \sqrt{1-\alpha_t\alpha_{t-1}} \epsilon $.
\end{tabbing}
Subtitusi $ \textbf{x}_{t-2} $:
\begin{tabbing} 
\hspace{4.75em} $ \coloneq \sqrt{\alpha_t \alpha_{t-1} \alpha_{t-2}} \textbf{x}_{t-3} + \sqrt{1-\alpha_t\alpha_{t-1}\alpha_{t-2}} \epsilon $.
\end{tabbing}
Subtitusi $ \textbf{x}_{t-3} $:
\begin{tabbing} 
\hspace{4.75em} $ \coloneq \sqrt{\alpha_t \alpha_{t-1} \alpha_{t-2} \alpha_{t-3}} \textbf{x}_{t-4} + \sqrt{1-\alpha_t\alpha_{t-1}\alpha_{t-2}\alpha_{t-3}} \epsilon $.
\end{tabbing}
Jika kita terus subtitusi  sampai dengan $ \textbf{x}_{1} $:
\begin{tabbing} 
\hspace{4.75em} $ \coloneq \sqrt{\alpha_t \alpha_{t-1} \alpha_{t-2} \alpha_{t-3} ... \alpha_{1}} \textbf{x}_{0} + \sqrt{1-\alpha_t\alpha_{t-1}\alpha_{t-2}\alpha_{t-3}...\alpha_{1}} \epsilon $.
\end{tabbing}
Diberikan definisi $ \bar{\alpha_t} \coloneq \displaystyle \prod_{t=1}^{T} \alpha_t $, maka:
\begin{equation}
q(\textbf{x}_{t}|\textbf{x}_{t-1}) \coloneq \sqrt{\bar{\alpha_t}} \textbf{x}_{0} + \sqrt{1-\bar{\alpha_t}} \epsilon.
\end{equation}

Penyederhaan $ q(\textbf{x}_{t}|\textbf{x}_{t-1}) $ menjadi persamaan (2) membuat rumus menjadi lebih mudah diselesaikan. Daripada melakukan iterasi dari $ \textbf{x}_{T} $ sampai $ \textbf{x}_{t-1} $, kita cukup menentukan $ \bar{\alpha_t} $, lalu subtitusi ke persamaan (2).


\subsection{Reverse Process}


Diffusion models adalah sebuah model latent variable dalam bentuk $ p_\theta(\textbf{x}_0) \coloneq \int p_\theta(\textbf{x}_{0:T}) \,d_{\textbf{x}_{1:T}} $, di mana $ \textbf{x}_1,\textbf{x}_2,\textbf{x}_3,...,\textbf{x}_T $ adalah latents dengan dimentionality yang sama dengan data $ \textbf{x}_0 \sim q(\textbf{x}_0) $. Joint distribution $ p_\theta(\textbf{x}_{0:T}) $ adalah reverse process:
\\\\
$ p_\theta(\textbf{x}_{0:T}) \coloneq p_\theta(\textbf{x}_0|\textbf{x}_1,\textbf{x}_2,\textbf{x}_3,...,\textbf{x}_T) \ p_\theta(\textbf{x}_1|\textbf{x}_2,\textbf{x}_3,\textbf{x}_4,...,\textbf{x}_T) \ ... \  p(\textbf{x}_{T}) $.
\\\\
Sama seperti forward process, reverse process didefinisikan sebagai Markov chain:
\begin{tabbing}
\hspace{3.75em} $ \coloneq p_\theta(\textbf{x}_0|\textbf{x}_1,\cancel{\textbf{x}_2},\cancel{\textbf{x}_3},\cancel{...},\cancel{\textbf{x}_T}) \ p_\theta(\textbf{x}_1|\textbf{x}_2,\cancel{\textbf{x}_3},\cancel{\textbf{x}_4},\cancel{...},\cancel{\textbf{x}_T}) \ ... \  p(\textbf{x}_{T})  $,
\end{tabbing}
sehingga kita mendapatkan persamaan akhir:
\\
\begin{equation}
p_\theta(\textbf{x}_{0:T}) \coloneq p(\textbf{x}_T) \displaystyle \prod_{t=1}^{T} p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t}).
\end{equation}
\\
Sejauh ini, kita tidak tahu distribusi asli dari reverse process. Oleh karena itu, kita menggunakan aproksimasi $ p_\theta $ yang nantinya akan dipelajari oleh neural network. Khusus untuk $ \textbf{x}_{T} $, kita tidak menggunakan aproksimasi $ p_\theta $ karena kita tahu nilai asli dari $ \textbf{x}_{T} $.

Reverse process merupakan Gaussian transition dengan $ \mu $ dan $ \sigma^2 $ yang akan dipelajari selanjutnya, dimulai dari $ p(\textbf{x}_T) = \mathscr{N}(\textbf{x}_T;0,\textbf{I}) $, yaitu pure noise dengan mean 0 dan variance 1 (matriks identitas):
\\
\begin{equation}
p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t}) \coloneq \mathscr{N}(\textbf{x}_{t-1};\mu_\theta(\textbf{x}_{t},t),\Sigma_\theta(\textbf{x}_t,t)).
\end{equation}
\\
Namun, author dari paper DDPM memutuskan untuk memberikan nilai yang pasti pada $ \sigma^2 $ berdasarkan linear schedule. Dengan kata lain, $ \sigma $ tidak perlu dipelajari pada reverse process.


\section{Loss Function}


Kita tidak mengetahui distribusi dari reverse process. Untuk itu, kita akan melatih model agar dapat melakukan aproksimasi distribusi dari p sebaik mungkin. Caranya cukup mirip pada paper Variational Autoencoder (VAE). Pada paper tersebut, kita tidak tahu distribusi asli dari x, diberikan distribusi Z. Untuk itu, kita akan mengaproksimasinya melalui neural network.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/VAE.png}
    \caption{aproksimasi x pada VAE}
    \label{fig:aproksimasi_x_pada_VAE}
\end{figure}
Di sini, kita ingin neural network agar dapat mengaproksimasi probability dari p of $ \textbf{x} $ sehingga dapat generate gambar yang semirip mungkin dengan distribusi  $ \textbf{x} $ (training data). Untuk itu, kita akan memaksimalkan log-likelihood dari $ p $. Fungsi log dibutuhkan agar nilai likelihood tidak exploding (terlalu besar hingga tidak bisa disimpan komputer). Fungsi log bersifat monotonic sehingga memaksimalkan log-likelihood berarti juga memaksimalkan true likelihood.
\begin{flalign*}
& \log p(x) = \log \int p(x,z) \,dz. &\\
& \hspace{3.9em} = \log \int p(x,z) \frac{q(z|x)}{q(z|x)} \,dz &\\
& \hspace{3.9em} = \log \mathbb{E}_{q(z|x)} \left[ \frac{p(x,z)}{q(z|x)} \right] &
\end{flalign*}
Logaritma merupakan fungsi concave. Hal ini dapat dibuktikan karena turunan kedua dari logaritma adalah negatif. Oleh sebab itu, berlaku: $ \log \mathbb{E}_{q(z|x)} \left[ \frac{p(x,z)}{q(z|x)} \right] = \mathbb{E}_{q(z|x)} \left[ \log \frac{p(x,z)}{q(z|x)} \right] $, sehingga:
\begin{flalign*}
& \log p(x) \geq \mathbb{E}_{q(z|x)} \left[\log \frac{p(x,z)}{q(z|x)} \right] &
\end{flalign*}

Kita dapat menerapkan hal yang sama pada diffusion models. Perbedaannya, kita tidak ke $ Z_T $, tetapi melalui urutan latent variable dari $ x_1 $ ke $ x_T $. Kita juga tidak lagi mencari loss function, melainkan ekspektasi dari loss function.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/reverse-process.png}
    \caption{reverse process}
    \label{fig:reverse_process}
\end{figure}
Disamping itu, kita tidak memaksimalkan log-likelihood, melainkan meminimalkan negative log-likelihood. Sebenarnya, kedua hal tersebut sama. Namun, optimizer biasanya meminimalkan fungsi sehingga akan lebih cocok jika kita menggunakan negative log-likelihood. Seperti namanya, yaitu loss function, kita ingin meminimalkan fungsi ini sekecil mungkin.
\begin{flalign*}
& 
-\log p_\theta(\textbf{x}_0) 
= -\log \int p_\theta(\textbf{x}_{0:T}) \,d\textbf{x}_{1:T}. 
&\\
& \hspace{5.4em} = -\log \int p_\theta(\textbf{x}_{0:T}) \frac{q(\textbf{x}_{1:T}|\textbf{x}_0)}{q(\textbf{x}_{1:T}|\textbf{x}_0)} \,d\textbf{x}_{1:T} &\\
& \hspace{5.4em} = -\log \int q(\textbf{x}_{1:T}|\textbf{x}_0) \frac{p_\theta(\textbf{x}_{0:T})}{q(\textbf{x}_{1:T}|\textbf{x}_0)} \,d\textbf{x}_{1:T} &\\
& \hspace{5.4em} = -\log \mathbb{E}_{q(\textbf{x}_{1:T}|\textbf{x}_0)} \left[ \frac{p_\theta(\textbf{x}_{0:T})}{q(\textbf{x}_{1:T}|\textbf{x}_0)} \right] &\\
& \hspace{5.4em} \leq \mathbb{E}_{q(\textbf{x}_{1:T}|\textbf{x}_0)} \left[ -\log \frac{p_\theta(\textbf{x}_{0:T})}{q(\textbf{x}_{1:T}|\textbf{x}_0)} \right] &\\
& \hspace{5.4em} \leq \mathbb{E}_{q(\textbf{x}_{1:T}|\textbf{x}_0)} \left[ \log \frac{q(\textbf{x}_{1:T}|\textbf{x}_0)}{p_\theta(\textbf{x}_{0:T})} \right] &
\end{flalign*}

Selanjutnya, kita dapat melakukan optimasi pada term yang berada di dalam ekspektasi.
\begin{flalign*}
& \log \frac{q(\textbf{x}_{1:T}|\textbf{x}_0)}{p_\theta(\textbf{x}_{0:T})} = \log \frac{\displaystyle \prod_{t=1}^{T} q(\textbf{x}_{t}|\textbf{x}_{t-1})}{p(\textbf{x}_T) \displaystyle \prod_{t=1}^{T} p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})} &
\end{flalign*}

$ q(\textbf{x}_{t}|\textbf{x}_{t-1}) $ dapat kita ubah menjadi $ \frac{q(\textbf{x}_{t-1}|\textbf{x}_t) \ q(\textbf{x}_t) }{ q(\textbf{x}_{t-1}) } $. Namun, $ q(\textbf{x}_{t-1}|\textbf{x}_t) $ akan memiliki variance yang tinggi karena kandidat untuk $ \textbf{x}_{t} $ bisa bermacam-macam.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{fig/high-variance-vs-low-variance.png}
    \caption{high variance vs. low variance}
    \label{fig:high-variance-vs-low-variance}
\end{figure}

Oleh karena itu, kita akan menambahkan gambar asli $ \textbf{x}_0 $ agar kandidat untuk $ \textbf{x}_{t-1} $ dapat menyesuaikan gambar aslinya.
\begin{tabbing}
\hspace{6em} $ = \log \frac{\displaystyle \prod_{t=1}^{T} q(\textbf{x}_{t}|\textbf{x}_{t-1},\textbf{x}_0) } { p(\textbf{x}_T) \displaystyle \prod_{t=1}^{T} p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})} $
\end{tabbing}

Namun, terdapat hal yang janggal. Jika kita teliti lebih dalam, term pertama pada numerator mengandung self-loop! Lebih tepatnya, $ q(\textbf{x}_1|\textbf{x}_0,\textbf{x}_0) = \frac{q(\textbf{x}_0|\textbf{x}_1,\textbf{x}_0) \ q(\textbf{x}_1|\textbf{x}_0) }{ q(\textbf{x}_0|\textbf{x}_0) } $. Untuk mengatasi hal ini, keluarkan term pertama dari product series. Setelah itu, kita baru bisa menambahkan $ \textbf{x}_0 $ pada conditional probability di product series.
\begin{tabbing}
\hspace{6em} $ =\xcancel{ \log \frac{\displaystyle \prod_{t=1}^{T} q(\textbf{x}_{t}|\textbf{x}_{t-1},\textbf{x}_0) } { p(\textbf{x}_T) \displaystyle \prod_{t=1}^{T} p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})} } $\\\\

\hspace{6em} $ = \log \frac{q(\textbf{x}_1|\textbf{x}_0) \displaystyle \prod_{t=2}^{T} q(\textbf{x}_{t}|\textbf{x}_{t-1},\textbf{x}_0) } { p(\textbf{x}_T) \displaystyle \prod_{t=1}^{T} p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})} $\\\\

\hspace{6em} $ = \log \frac{q(\textbf{x}_1|\textbf{x}_0) \displaystyle \prod_{t=2}^{T} q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0) \ q(\textbf{x}_{t}|\textbf{x}_0) } { q(\textbf{x}_{t-1}|\textbf{x}_0) \ p(\textbf{x}_T) \displaystyle \prod_{t=1}^{T} p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})} $\\\\

\hspace{6em} $ = \log \frac{ 
\cancel{q(\textbf{x}_1|\textbf{x}_0)}
\ q(\textbf{x}_{T-1}|\textbf{x}_T,\textbf{x}_0) \ q(\textbf{x}_T|\textbf{x}_0) 
\ q(\textbf{x}_{T-2}|\textbf{x}_{T-1},\textbf{x}_0) \ \cancel{q(\textbf{x}_{T-1}|\textbf{x}_0)} 
\ ...
\ q(\textbf{x}_{1}|\textbf{x}_{2},\textbf{x}_0) \ \cancel{q(\textbf{x}_{2}|\textbf{x}_0)} 
} 
{ 
\cancel{q(\textbf{x}_{T-1}|\textbf{x}_0)} 
\ q(\textbf{x}_{T-2}|\textbf{x}_0)
\ ...
\ \cancel{q(\textbf{x}_{2}|\textbf{x}_0)}
\ \cancel{q(\textbf{x}_{1}|\textbf{x}_0)}
\ p(\textbf{x}_T) \displaystyle \prod_{t=1}^{T} p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t}) } $\\\\

\hspace{6em} $ = \log \frac{
q(\textbf{x}_T|\textbf{x}_0) \displaystyle \prod_{t=2}^{T} q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0)
} 
{ 
p(\textbf{x}_T) \displaystyle \prod_{t=1}^{T} p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})
} $\\\\

\hspace{6em} $ = \log \frac{
q(\textbf{x}_T|\textbf{x}_0) \displaystyle \prod_{t=2}^{T} q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0)
} 
{ 
p(\textbf{x}_T) \ p_\theta(\textbf{x}_0|\textbf{x}_1) \displaystyle \prod_{t=2}^{T} p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})
} $\\\\

\hspace{6em} $ \displaystyle = \log \frac{
q(\textbf{x}_T|\textbf{x}_0)
} 
{ 
p(\textbf{x}_T)
} + 
\sum_{t=2}^{T} \log \frac{
q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0)
}
{
p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})
} -
\log p_\theta(\textbf{x}_0|\textbf{x}_1)
$
\end{tabbing}

Masukkan kembali $ \log \frac{q(\textbf{x}_{1:T}|\textbf{x}_0)}{p_\theta(\textbf{x}_{0:T})} $ pada loss function:
\begin{flalign*}
& -\log p_\theta(\textbf{x}_0) \leq \mathbb{E}_{q(\textbf{x}_{1:T}|\textbf{x}_0)} \left[ 
\log \frac{
q(\textbf{x}_T|\textbf{x}_0)
} 
{ 
p(\textbf{x}_T)
} + 
\sum_{t=2}^{T} \log \frac{
q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0)
}
{
p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})
} -
\log p_\theta(\textbf{x}_0|\textbf{x}_1)
\right]. &\\
\end{flalign*}

Ubah setiap term pada loss function ke bentuk KL (Kullbackâ€“Leibler divergence) divergence:
\begin{flalign*}
& 
\mathbb{E}_{q(\textbf{x}_{1:T}|\textbf{x}_0)} \left[ 
\log \frac{
q(\textbf{x}_T|\textbf{x}_0)
} 
{ 
p(\textbf{x}_T)
} + 
\sum_{t=2}^{T} \log \frac{
q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0)
}
{
p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})
} -
\log p_\theta(\textbf{x}_0|\textbf{x}_1)
\right] 
&\\
& 
\hspace{4em} = 
\mathbb{E}_{q(\textbf{x}_{1:T}|\textbf{x}_0)} \left[ 
\infdiv{q(\textbf{x}_T|\textbf{x}_0)}{p(\textbf{x}_T)}
+ 
\sum_{t=2}^{T} \infdiv{ 
q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0)
}
{
p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})
}
-
\log p_\theta(\textbf{x}_0|\textbf{x}_1)
\right]
&
\end{flalign*}

Didapatkan:
\\
\begin{equation}
{\small L = 
\mathbb{E}_{q} \left[ 
\infdiv{q(\textbf{x}_T|\textbf{x}_0)}{p(\textbf{x}_T)}
+ 
\sum_{t=2}^{T} \infdiv{ 
q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0)
}
{
p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})
}
-
\log p_\theta(\textbf{x}_0|\textbf{x}_1)
\right]
}
\end{equation}
\\
dengan 
\begin{equation}
q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0) = \mathscr{N}(\textbf{x}_{t-1};\tilde{\mu}_t(\textbf{x}_{t},\textbf{x}_0),\tilde{\beta}_t\textbf{I}), 
\end{equation}
\\
\begin{equation}
\tilde{\mu}_t(\textbf{x}_{t},\textbf{x}_0) \coloneq \frac{
\sqrt{\bar{\alpha}_{t-1}}\beta_t
}
{
1 - \bar{\alpha}_t
}
\textbf{x}_0 
+
\frac{
\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})
}
{
1 - \bar{\alpha_t}
}
\textbf{x}_t,
\end{equation}
\\
\begin{equation}
\tilde{\beta}_t \coloneq 
\frac{
1-\bar{\alpha}_{t-1}
}
{
1-\bar{\alpha}
}
\beta_t.
\end{equation}

Kita dapat menguraikan L menjadi:
\\
\begin{equation}
L_T = 
\infdiv{q(\textbf{x}_T|\textbf{x}_0)}{p(\textbf{x}_T)}.
\end{equation}
\\
\begin{equation}
L_{t-1} = 
\infdiv{ 
q(\textbf{x}_{t-1}|\textbf{x}_t,\textbf{x}_0)
}
{
p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t})
}.
\end{equation}
\\
\begin{equation}
L_{0} = 
\log p_\theta(\textbf{x}_0|\textbf{x}_1).
\end{equation}
\\


\subsection{Forward process dan $ L_T $}
Sebenarnya, $ \beta_t $ dapat dipelajari melalui reparameterization. Namun, nilai $ \beta_t $ akan dibuat tetap menjadi konstanta sehingga q tidak ada parameter yang dapat dipelajari. Hal ini membuat $ L_T $ menjadi konstan selama training dan dapat diabaikan.


\subsection{Reverse process dan $ L_{1:T-1} $}
Kita ingin mempelajari mean dan variance dari reverse diffusion process $ p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t}) = \mathscr{N}(\textbf{x}_{t-1};\mu_\theta(\textbf{x}_{t},t),\Sigma_\theta(\textbf{x}_t, t)) $, dengan $ 1 < t < T  $. Di sini, author memutuskan untuk tidak mempelajari variance dan menjadikannya konstanta:
\\
\begin{equation}
\Sigma_\theta(\textbf{x}_t, t) = \sigma_t^2 \textbf{I}
\end{equation}
\\
sehingga
\\
\begin{equation}
 p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t}) = \mathscr{N}(\textbf{x}_{t-1};\mu_\theta(\textbf{x}_{t},t),\sigma_t^2 \textbf{I})
\end{equation}
\\

Untuk merepresentasikan mean $ \mu_\theta(\textbf{x}_{t},t) $, penulis DDPM melakukan parameterisasi berdasarkan analisis terhadap $ L_t $, yaitu $
\infdiv{ 
q(\textbf{x}_{t}|\textbf{x}_{t+1},\textbf{x}_0)
}
{
p_\theta(\textbf{x}_{t}|\textbf{x}_{t+1})
}
$, dengan menggunakan mean square error:
\begin{equation}
L_{t-1} = \mathbb{E}_{q} \left[ 
\frac{1}{2\sigma_t^2}
\norm{
\tilde{\mu}_t(\textbf{x}_{t},\textbf{x}_0) - \mu_\theta(\textbf{x}_{t},t)
}^2
\right] + C,
\end{equation}
dengan C adalah konstanta yang tidak bergantung pada $ \theta $. Subtitusi $ \tilde{\mu}_t(\textbf{x}_{t},\textbf{x}_0) $ pada (12):
\begin{flalign*}
L_{t-1} = \mathbb{E}_{q} \left[ 
\frac{1}{2\sigma_t^2}
\norm{
\frac{
\sqrt{\bar{\alpha}_{t-1}}\beta_t
}
{
1 - \bar{\alpha}_t
}
\textbf{x}_0 
+
\frac{
\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})
}
{
1 - \bar{\alpha_t}
}
\textbf{x}_t 
-
 \mu_\theta(\textbf{x}_{t},t)
}^2
\right] + C,
\end{flalign*}

Pada persamaan (2), kita telah diperlihatkan bagaimana mencari $ \textbf{x}_t $ jika diberikan $ \textbf{x}_0 $. Persamaan tersebut dapat kita gunakan untuk mencari $ \textbf{x}_0 $:
\begin{flalign*}
& \textbf{x}_t = \sqrt{\bar{\alpha_t}} \textbf{x}_{0} + \sqrt{1-\bar{\alpha_t}} \epsilon &\\
& \iff \textbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha_t}}}(\textbf{x}_{t} - \sqrt{1-\bar{\alpha}_t} \epsilon) &
\end{flalign*}
Subtitusi $ \textbf{x}_0 $ pada (14):
\begin{flalign*}
&
L_{t-1} = \mathbb{E}_{q} \left[ 
\frac{1}{2\sigma_t^2}
\norm{
\frac{
\sqrt{\bar{\alpha}_{t-1}}\beta_t
}
{
1 - \bar{\alpha}_t
}
\frac{1}{\sqrt{\bar{\alpha_t}}}(\textbf{x}_{t} - \sqrt{1-\bar{\alpha}_t} \epsilon)
+
\frac{
\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})
}
{
1 - \bar{\alpha_t}
}
\textbf{x}_t 
-
\mu_\theta(\textbf{x}_{t},t)
}^2
\right] + C 
&\\
&
\hspace{2em} = \mathbb{E}_{q} \left[ 
\frac{1}{2\sigma_t^2}
\norm{
\frac{1}{\sqrt{\bar{\alpha}_t}}(\textbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon)
-
\mu_\theta(\textbf{x}_{t},t)
}^2
\right] + C 
&
\end{flalign*}

Penulis DDPM mereparameterisasi $ x_t $ agar dapat memprediksi noise pada reverse diffusion process:
\begin{equation}
\hspace{2em} = \mathbb{E}_{q} \left[ 
\frac{1}{2\sigma_t^2}
\norm{
\frac{1}{\sqrt{\bar{\alpha}_t}} ( \textbf{x}_t(\textbf{x}_0,\epsilon ) - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon)
-
\mu_\theta(\textbf{x}_t(\textbf{x}_0,\epsilon),t)
}^2
\right] + C,
\end{equation}
dengan $ \textbf{x}_t(\textbf{x}_0,\epsilon) = \sqrt{\bar{\alpha}_t}\textbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, \hspace{1em} \epsilon \sim\mathscr{N}(0,\textbf{I}). $
Persamaan (10) memperlihatkan bahwa $ \mu_\theta $ harus memprediksi $ \frac{1}{\sqrt{\bar{\alpha}_t}} ( \textbf{x}_t(\textbf{x}_0,\epsilon ) - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon) $, diberikan $ \textbf{t} $. Karena $ \textbf{t} $ sudah ada sebagai input, kita dapat melakukan parameterisasi sebagai berikut:
\begin{flalign*}
& \mu_\theta(\textbf{x}_{t},t) = \bar{\mu}_t(\textbf{x}_{t}, \textbf{x}_0(\textbf(x)_t, \epsilon_\theta)) &\\
& \hspace{2em} = \bar{\mu}_t(\textbf{x}_{t}, \frac{1}{\sqrt{\bar{\alpha_t}}}(\textbf{x}_{t} - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta(\textbf{x}_t))) &
\end{flalign*}
\begin{fleqn}[\parindent]
\begin{equation}
\begin{split}
\hspace{2.25em} = \frac{1}{\sqrt{\bar{\alpha}_t}} ( \textbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(\textbf{x}_t, t))
\end{split}
\end{equation}
\end{fleqn}

Subtitusi persamaan (16) pada persamaan (15):
\begin{flalign*}
& L_{t-1} = \mathbb{E}_{q} \left[ 
\frac{1}{2\sigma_t^2}
\norm{
\frac{1}{\sqrt{\bar{\alpha}_t}} ( \textbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon)
-
\mu_\theta(\textbf{x}_t,t)
}^2
\right] + C 
&\\
&
\hspace{2.25em} = \mathbb{E}_{q} \left[ 
\frac{1}{2\sigma_t^2}
\norm{
\frac{1}{\sqrt{\bar{\alpha}_t}} ( \textbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon)
-
\frac{1}{\sqrt{\bar{\alpha}_t}} ( \textbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(\textbf{x}_t, t))
}^2
\right] + C 
&
\end{flalign*}
\begin{fleqn}[\parindent]
\begin{equation}
\begin{split}
\hspace{2.25em} = \mathbb{E}_{q} \left[ 
\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}
\norm{
\epsilon
-
\epsilon_\theta(\textbf{x}_t, t)
}^2
\right] + C
\end{split}
\end{equation}
\end{fleqn}

Setelah melakukan percobaan, ternyata penulis DDPM berpendapat bahwa menghilangkan term pertama pada (17) pada proses training akan meningkatkan kualitas sample dan juga lebih mudah dalam implementasi:
\begin{equation}
L_{simple}(\theta) \coloneq \mathbb{E}_{q} \left[ 
\norm{
\epsilon
-
\epsilon_\theta(\textbf{x}_t, t)
}^2
\right]
\end{equation}

Recall $ p_\theta(\textbf{x}_{t-1}|\textbf{x}_{t}) = \mathscr{N}(\textbf{x}_{t-1};\mu_\theta(\textbf{x}_{t},t),\Sigma_\theta(\textbf{x}_t, t)) $, kita dapat mensample $ \textbf{x}_{t-1} $ melalui reparameterization trick $ \mathscr{N}(\mu,\sigma^2) = \mu + \sigma \epsilon $:
\begin{flalign*}
& \textbf{x}_{t-1} = \mu_\theta(\textbf{x}_{t},t) + \Sigma_\theta(\textbf{x}_{t},t) + \epsilon &
\end{flalign*}
\begin{fleqn}[\parindent]
\begin{equation}
\begin{split}
\hspace{2.25em} = \frac{1}{\sqrt{\bar{\alpha}_t}} ( \textbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(\textbf{x}_t, t)) + \sqrt{\beta_t} \epsilon
\end{split}
\end{equation}
\end{fleqn}


\section{Data Scaling}


\end{document}